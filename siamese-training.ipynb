{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0295a5309c224cfa8147cc6381ba0492",
      "f0b7c381163441f3823cc5c7c789367c",
      "ebb78b70b7ed4ce3a31791dd0ebf3795",
      "be7b735ab5b14fa6990c067244c574bc",
      "811d6e79bf9a4225a8cd65433bdd3b50",
      "ca4b2b654be6477195e28172d4298449",
      "3aec49a4b7af4a9e8015f9696c2482f7",
      "745d16cbb4874cb9862d0721d7d79288",
      "6694b21b8d5749d8af0c87567ba33207",
      "810b5174595a45319ae3ab5cbb935f05",
      "97671c79568c49508b267f0b07514e68",
      "a8a9e65d4fbf446fa6a88ab4c61b2d8a",
      "da1f43970fda427b811272078f8efd42",
      "ec1c8b677bae4ddc9cb06491794111ae",
      "2c10548586e6461c86616aaa64fb5507",
      "66167462ecd447108570b9adfe5b220d",
      "88ff205ad4b84f138526e3ba59f0625f",
      "7f77995af238420c99a2329b5a6a0512",
      "e90417a5d2da428d8ce0fb3b23ae80d3",
      "31b702c6c309401c8424c0133f56e3aa",
      "f4e76d8d165147dd8ff94c4e46b738ce",
      "48f576081f694233b0a1543299a66e6e",
      "f9e0bedbb39b4ccbbf100a49d3ed7244",
      "fe4efda1ba51416d85b9ebe37a51a7ed",
      "27401c222c634a239eb0358ba7762f81",
      "f301485013474fdb95440c2629f38962",
      "1b4063da094345c289bd708f02cd1ecd",
      "9d6a7833e18f40a28157166341016c14",
      "2cbe282fe9984e31abf20b74f283fbd4",
      "a38748ae35f748a582b1e9b96f5fd343",
      "d768746174eb4699b2c5c321cd65f843",
      "ae635c6fa1054557b7ce88f62e319885",
      "8816d71662424bc9aa77d2af805bb985",
      "1e71921d5d3d43d1b0646737ce2298c5",
      "3f55f8f3135843e193833fbf6eea8869",
      "b0df3fac10b54f96b7eb5a9cbd0c93b6",
      "bdc6805674f743f2a09b51bf67538c7f",
      "828894f53d4f4b4ca2bca35baa7bec80",
      "0364cfe0aef6495aac43f8bee945ee9d",
      "a166605298d64cc1b8788ac2ca0d8c80",
      "28038eecbfd2408dba28d8cfcb7c3140",
      "ef1cc576e16d4e318c9f56a1253c5d5b",
      "9dc9e998dba5431991ab4e540ed3c99e",
      "630270fcef4d479d9226fde3b91a04de",
      "1e9cace16f624a8d8552a2814fed12c0",
      "95fcab413c254e8292350be67b38a64a",
      "e14b35b615a34ccca978795ccaafc757",
      "e11701d46076461c8d97a2ee406f2a14",
      "0d245a76af7249e59a83b4e49dbf6c6a",
      "826bea647e024f9eb8c41a1ff16bc09c",
      "07fb32148f1d4351a019cb82cb0c31bf",
      "3e4815817252450a8d98735cccb18562",
      "bf0b5d25ed5d4345970246eaea8c8850",
      "1487686e98604f9faee5c54c19a21118",
      "e72b4992958541d78da40aae12441c32",
      "534d9a7c9ef14faeb053c77af3c750a1",
      "c88d0bc271c246ff831f079a4d5ae039",
      "1b1ca778baac47b8b385b577159d5c7a",
      "758f99aa0192400fadc14d68435dde70",
      "a9683c9f9b2545ec80d44d5002c16b38",
      "e12db6799367430ba85dd6f0b543a92d",
      "032ee0e0d4354ad8936b8c18894d337c",
      "754a86ad647e4802b55ffef5e03a81c4",
      "2752db9f773845eb8add1961da2bba4d",
      "4128694e7677493c87411f51c9655b42",
      "820c89dc878f403b87adb8f7983ca8aa",
      "ab555d0c466046ab88d466fde1d55f85",
      "280d9cbce77146adb5fa02c8763d2176",
      "7d0e139a87fb4cc08122a53ae107e80c",
      "797e5584e3e74e81823604b78fe9fc03",
      "0315ec1884784984807d93daf948c650",
      "7b0f6ae18c054b89a44d5485b1b630e6",
      "fe0ba2e2c4774d3089027fedadf909f0",
      "079dac0d013844db9ba318889246334c",
      "2b3992f30f6b4a37894cea00fa0ba26a",
      "4c964f1a54a544d59067d19fcda9e250",
      "6ee51ac3b9584e64ab9a34933cb953f2",
      "a19280f59746471cb03076286d574a2b",
      "04ff2ed557b14cfda7b0383c6e3c288d",
      "cf4d53fd3466420484f3b09bb3c87547",
      "2ab07a5efd2b4a6e8ccba4f3c1cd627d",
      "24561d1f9f4847f6ae03692755f5f285",
      "bfd2539cb27b4c5b91c1bd479addc13f",
      "3dbee62992e14905aa9d7c7a3e3774ad",
      "a97d03dc53024e12863a290bab78242d",
      "195ea02572be402fb6df63e8a86baeea",
      "addabed9d1f340a6934b45e3d062285e",
      "8dde20735bfe436c8d56083e0073dd02"
     ]
    },
    "id": "uYf4YDXMIAHK",
    "outputId": "f5333042-0c06-4bfb-e2fd-37c72975aeaa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "\n",
    "\n",
    "def create_image_pairs(dataset, labels):\n",
    "    \"\"\"Creates positive and negative pairs of images.\"\"\"\n",
    "    digit_indices = [np.where(labels == i)[0] for i in range(10)]\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "\n",
    "    for idx1 in range(len(dataset)):\n",
    "        # Get the current image and its label\n",
    "        x1 = dataset[idx1]\n",
    "        label1 = labels[idx1]\n",
    "\n",
    "        # Add a positive pair\n",
    "        idx2 = np.random.choice(digit_indices[label1])\n",
    "        x2 = dataset[idx2]\n",
    "        pairs.append([x1, x2])\n",
    "        pair_labels.append(1.0)\n",
    "\n",
    "        # Add a negative pair\n",
    "        label2 = np.random.randint(0, 10)\n",
    "        while label2 == label1:\n",
    "            label2 = np.random.randint(0, 10)\n",
    "        idx2 = np.random.choice(digit_indices[label2])\n",
    "        x2 = dataset[idx2]\n",
    "        pairs.append([x1, x2])\n",
    "        pair_labels.append(0.0)\n",
    "\n",
    "    return np.array(pairs), np.array(pair_labels)\n",
    "\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(train_ds, test_ds), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Normalize and extract images and labels\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "train_dataset = train_ds.map(normalize_img)\n",
    "test_dataset = test_ds.map(normalize_img)\n",
    "\n",
    "train_images = np.array([x for x, y in train_dataset])\n",
    "train_labels = np.array([y for x, y in train_dataset])\n",
    "test_images = np.array([x for x, y in test_dataset])\n",
    "test_labels = np.array([y for x, y in test_dataset])\n",
    "\n",
    "# Create training and validation pairs\n",
    "train_pairs, train_y = create_image_pairs(train_images, train_labels)\n",
    "test_pairs, test_y = create_image_pairs(test_images, test_labels)\n",
    "\n",
    "print(f\"Training pairs shape: {train_pairs.shape}\")\n",
    "print(f\"Training labels shape: {train_y.shape}\")\n",
    "\n",
    "# --- 2. Build the Siamese Network ---\n",
    "\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    \"\"\"Creates the base CNN model that produces the embeddings.\"\"\"\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(input)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    return tf.keras.Model(input, x)\n",
    "\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"Calculates the euclidean distance between two vectors.\"\"\"\n",
    "    x, y = vects\n",
    "    sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "input_shape = (28, 28, 1)\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = tf.keras.layers.Input(shape=input_shape)\n",
    "input_b = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# Process both inputs with the same base network\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Calculate the distance between the embeddings\n",
    "distance = tf.keras.layers.Lambda(euclidean_distance)([processed_a, processed_b])\n",
    "\n",
    "model = tf.keras.Model([input_a, input_b], distance)\n",
    "model.summary()\n",
    "\n",
    "# --- 3. Contrastive Loss Function ---\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\"Contrastive loss function.\n",
    "\n",
    "    y_true: label (1 for similar, 0 for dissimilar)\n",
    "    y_pred: predicted distance from the model\n",
    "    \"\"\"\n",
    "    margin = 1.0\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "# --- 4. Training and Evaluation ---\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_pairs[:, 0], train_pairs[:, 1]],\n",
    "    train_y,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=([test_pairs[:, 0], test_pairs[:, 1]], test_y),\n",
    ")\n",
    "\n",
    "# --- 5. Visualize Results ---\n",
    "\n",
    "\n",
    "def display_image_pairs(pairs, labels, predictions, num_items=10):\n",
    "    \"\"\"Displays pairs of images, their true label, and the predicted distance.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_items):\n",
    "        ax = plt.subplot(2, num_items, i + 1)\n",
    "        plt.imshow(pairs[i, 0].squeeze(), cmap=\"gray\")\n",
    "        plt.title(f\"Label: {int(labels[i])}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        ax = plt.subplot(2, num_items, num_items + i + 1)\n",
    "        plt.imshow(pairs[i, 1].squeeze(), cmap=\"gray\")\n",
    "        plt.title(f\"Pred: {predictions[i][0]:.2f}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get predictions on test pairs\n",
    "predictions = model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "\n",
    "# Select random pairs to display\n",
    "random_indices = np.random.choice(len(test_y), size=10)\n",
    "display_image_pairs(\n",
    "    test_pairs[random_indices], test_y[random_indices], predictions[random_indices]\n",
    ")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "MDDUnyjoKe9V",
    "outputId": "42def1f6-8dfe-4776-92ef-c95471006c12"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a smaller subset of the test data for faster testing\n",
    "\n",
    "test_pairs_subset = test_pairs[:1000]\n",
    "\n",
    "test_y_subset = test_y[:1000]\n",
    "\n",
    "\n",
    "# Get predictions on the subset of test pairs\n",
    "\n",
    "predictions_subset = model.predict([test_pairs_subset[:, 0], test_pairs_subset[:, 1]])\n",
    "\n",
    "\n",
    "# Select random pairs from the subset to display\n",
    "\n",
    "random_indices_subset = np.random.choice(len(test_y_subset), size=10)\n",
    "\n",
    "display_image_pairs(\n",
    "    test_pairs_subset[random_indices_subset],\n",
    "    test_y_subset[random_indices_subset],\n",
    "    predictions_subset[random_indices_subset],\n",
    ")\n",
    "\n",
    "\n",
    "# You can also evaluate the loss on the test subset\n",
    "\n",
    "loss_subset = model.evaluate(\n",
    "    [test_pairs_subset[:, 0], test_pairs_subset[:, 1]], test_y_subset\n",
    ")\n",
    "\n",
    "print(f\"Loss on test subset: {loss_subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFEkFbMgLPyT"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"siamese_mnist_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKaiuiGrLr22"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
